---
title: 'HarvardX - PH125.9x - DataScience: Capstone - MovieLens'
author: "Jan Brettschneider"
date: "29th March 2020"
output: pdf_document
---
## 1. Introduction
This document summarizes the MovieLens project for the Capstone Exam in course of the HardvardX Data Science Certificate.

The dataset used is the **10M version of the MovieLens dataset** which contains 10 million ratings from different users for different movies. We will examine the dataset in more depth in the analysis section.

Netflix offered a challenge for improving the algorithm of rating prediction, with a grand prize of US$ 1,000,000 up for grabs. The winning team bested the Netflix algorithm by 10,06%. The dataset used for this challenge was approximately 10 times larger than the one we use for this project.

During this project the following steps will be conducted:

  1. Create/download the data
  2. Explore the data and find variables which seem useful for the model creation
  3. Build and optimize the model with help of the parameters identified and regularization.
  4.Use the best model for validation and discuss the results
  5. Conclusion, limitations and comments
  
The ambition is to get a value below 0.86490.

### The dataset
The course staff provides the following code to download and manipulate the dataset, which will be used as an input for the project:

```{r get_data, message=FALSE, warning=FALSE}
################################
# Create edx set, validation set
################################

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding")
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)

```

Two datasets have been created. The edx dataset contains 9,000,055 observations, the validation set 999,999 observations.
Each dataset contains the following variables:
* **userId:** Integer variable for identifying the user for the rating
* **movieId:** Numeric unique indentifier for the movie
* **rating:** Numeric variable from 0 to 5 in steps of 0.5 whereby 5 represents the best rating
* **timestamp:** Integer variable giving the time of rating, which has to be translated first. Base is 1970-01-01
* **title:** Char, which contains the title of the movie with the release year in brackets behind
* **genre:** Char, containing one or more genres seperated by "|"

### Manipulating the dataset before the analysis
Before we can work with the dataset we have to perform some operations to manipulate the data but also to make computation (especially RAM usage) more efficient.
Therefore we will transform **userId**, **movieId** and **genres** into factors instead of the afore mentioned data types.
The timestamp will be transformed into the date. In this process we will reduce it to the year instead of a full date plus time.
From the title column the year in which the movie has been released is extracted, put into a new column and the title is adapted accordingly. This gives us the possibility to access the this metric much more easy. Note that we do the computation on both sets, because we want the possibility to use the new created variables for the prediction later. This is not a unallowed use of the validation dataset because we are not testing our model!

```{r manipulate_data, message=FALSE, warning=FALSE}
# Factorize userId, movieId and genre
edx$userId <- as.factor(edx$userId)
edx$movieId <- as.factor(edx$movieId)
edx$genres <- as.factor(edx$genres)
validation$userId <- as.factor(validation$userId)
validation$movieId <- as.factor(validation$movieId)
validation$genres <- as.factor(validation$genres)

# Convert the timestamp an extracting the year of rating
# starting at the 1st of January 1970
edx$timestamp <- year(as.POSIXct(edx$timestamp, origin = "1970-01-01"))
validation$timestamp <- year(as.POSIXct(validation$timestamp, origin = "1970-01-01"))

# Extract the release year of the movie from the title - 
# format: xxx (yyyy) and rename the title without year
edx$year<-substr(edx$title,nchar(as.character(edx$title))-4,
                  nchar(as.character(edx$title))-1)
edx$title<-paste0(substr(edx$title,1,nchar(as.character(edx$title))-6))
validation$year<-substr(validation$title,nchar(as.character(validation$title))-4,
                  nchar(as.character(validation$title))-1)
validation$title<-paste0(substr(validation$title,1,
                  nchar(as.character(validation$title))-6))

# Adding a unique identifier for each rating
edx <- edx %>% mutate(ratingId = row_number())
validation <- validation %>% mutate(ratingId = row_number())
```

Now the dataset looks like this:
```{r explore_data}
head(edx)
```  


## 2. Analysing the dataset
In this section we will have a look at the distribution of the variables and will get some insights which variables might be a good input for our model. Let's look at the distribution of the ratings. 

Before we look at the ratings we summarize the ratings data for the key metrics which will help to decide which factors to test for out model.
```{r summarize_ratings}
summary(edx$rating)
```  

First we will take a look at how the ratings are distributed.
```{r rating_distribution}
edx %>%
  group_by(rating) %>%
  summarize(n = n()) %>%
  ggplot(aes(rating, n)) +
  geom_bar(stat = "identity") +
  labs(title = "Distribution of the ratings",
       x = "Rating",
       y = "Frequency")
```
We can see, that the ratings do vary significantly, as expected. High ratings seem to be more common than low ratings and full digits like 3.0 or 4.0 more common than 2.5 or 3.5 for example.


The following script will show us how the average rating per user is distributed:
```{r rating_per_user}
edx %>%
  group_by(userId) %>%
  summarize(rating = round(mean(rating),1)) %>%
  ungroup() %>%
  group_by(rating) %>%
  summarize(n = n()) %>%
  ggplot(aes(rating, n)) +
  geom_bar(stat = "identity") +
  labs(title = "Average rating per user distribution",
       x = "Average Rating for user",
       y = "Frequency")
```  
Looking at the average rating users are giving, also some variance can be found although it almost looks like a bell curve. Average ratings per user seem to be to a large amount in the range of 3.0 to 4.0.


We can do the same for the average rating per movie:
```{r rating_per_movie}
edx %>%
  group_by(movieId) %>%
  summarize(rating = round(mean(rating),1)) %>%
  ungroup() %>%
  group_by(rating) %>%
  summarize(n = n()) %>%
  ggplot(aes(rating, n)) +
  geom_bar(stat = "identity") +
  labs(title = "Average rating per movie distribution",
       x = "Average Rating for user",
       y = "Frequency")
```  
The distribution for the average rating per movie seems to be distributed more widely, which indicates that we should take the individual movie into consideration when creating the model.

Does the year in which the movie has been published have an impact?:
```{r rating_per_year}
edx %>%
  group_by(year) %>%
  summarize(rating = mean(rating)) %>%
  ggplot(aes(year, rating)) +
  geom_bar(stat = "identity") +
  labs(title = "Average rating depending on the release year of the movie",
       x = "Year of release",
       y = "Average rating") +
 theme(axis.text.x = element_text(angle = 90, hjust = 1))
```
The year in which the movie has been published seems to have a slight influence also. Newer movies (1980 and later) seem to be slightly lower rated than earlier movies, although the effect is not as strong as for the other two variables.

How about the year of the rating? Can we see some variance here?:
```{r rating_per_timestamp}
edx %>%
  group_by(timestamp) %>%
  summarize(rating = mean(rating)) %>%
  ggplot(aes(timestamp, rating)) +
  geom_bar(stat = "identity") +
  labs(title = "Average rating depending on the year of rating",
       x = "Year of rating",
       y = "Average rating") +
 theme(axis.text.x = element_text(angle = 90, hjust = 1))
```
The year in which the rating has been done seems to have no significant effect on the rating. Except for the higher rating in 1995 all other years seem to be relatively stable around 3.5 our average value.

A bit more complicated is the examination of the genres, due to the fact, that often more than one is listed for each movie. However we can do it like that. First we split up the ratings with more than one genre:
```{r rating_per_genre}
temp <- edx %>% separate_rows(genres, sep = "\\|")
```

We can see that we have 19 unique genres and some movies without genre:
```{r number_of_genres}
length(unique(temp$genres))
```

The average rating per genre and movies per genre are the following:
```{r genre_overview}
temp %>% group_by(genres) %>% summarize(rating = mean(rating), n = n()) %>% arrange(rating)
```

A visual representation of the rating per genre:
```{r genre_visual}
temp %>% group_by(genres) %>% summarize(rating = mean(rating)) %>%
  ggplot(aes(genres, rating)) +
  geom_bar(stat = "identity")+
  labs(title = "Average rating per genre",
       x = "Genre",
       y = "Average rating") +
 theme(axis.text.x = element_text(angle = 90, hjust = 1))
```
The data analysis suggests, that **movieId**, **userId** and **release year** seem to have the most significant impact on the rating. In addition the more complex genres column is worth a try. For this we need to seperate the genres column into several rows and put it together after model building and estimation. More in the model building section.

## 3. Model building
In this section we will follow a stepwise approach for bulding the model. For testing the results we will use the RMSE function provided in course of this lecture.
```{r RMSE function}
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}
```

### 3.1 Creating the datasets
Defining test and train sets as well as splitting up all datasets by genres for including them into the model
```{r create_datasets}
# Define the index for train and test set 
train_index <- sample(1:nrow(edx), 0.8 * nrow(edx))
test_index <- setdiff(1:nrow(edx), train_index)

# Create train and test set
train_set <- edx[train_index,]
test_set <- edx[test_index,]

# Make sure we don't have users and movies in the test set
# which are not in the training set
test_set <- test_set %>%
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")

# Seperating the genres
edx_split <- edx %>% separate_rows(genres, sep = "\\|")
train_set_split <- train_set %>% separate_rows(genres, sep = "\\|")
test_set_split <- test_set %>% separate_rows(genres, sep = "\\|")
```


### 3.2 Models without regularization
We start with just taking the average rating across the dataset. **mu** contains the mean of all ratings.
```{r average_model, warning=FALSE}
mu <- mean(train_set$rating)

# Predict ratings
predicted_ratings <- test_set %>%
    mutate(pred = mu)

# Calculate the RMSE
RMSE_mu <- RMSE(predicted_ratings$pred, test_set$rating)
```

```{r table_1, echo = FALSE, warning=FALSE}
result_table <- data_frame(Method = "Average", RMSE = RMSE_mu)
result_table %>% knitr::kable(caption = "RMSEs of models")
```

Next we add the movie effect to the model. We do that by using **b_i** as the mean rating difference of each movie to the mean ratings. We then predict the rating by adding the **b_i** term to our calculation.
```{r movie_effect_model, warning=FALSE}
# Movie effect without regularization 
b_i <- train_set %>%
  group_by(movieId) %>%
  summarize(b_i = mean(rating - mu))

# Predict ratings
predicted_ratings <- test_set %>%
  left_join(b_i, by = "movieId") %>%
  mutate(pred = mu + b_i)

# Calculate the RMSE 
RMSE_movie <- RMSE(predicted_ratings$pred, test_set$rating)
```

```{r table_2, echo = FALSE, warning=FALSE}
result_table <- rbind(result_table,
                data_frame(Method = "Movie Effect", RMSE = RMSE_movie))
result_table %>% knitr::kable(caption = "RMSEs of models")
```

We can see that we get a significant improvment over our baseline by just adding the movie effect to the model.

The same can be done for the user effect:
```{r user_effect_model, warning=FALSE}
# User effect without regularization 
b_u <- train_set %>%
  left_join(b_i, by = "movieId") %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))
  
# Predict ratings
predicted_ratings <- test_set %>%
  left_join(b_i, by = "movieId") %>%
  left_join(b_u, by = "userId") %>%
  mutate(pred = mu + b_i + b_u)

# Calculate the RMSE
RMSE_user <- RMSE(predicted_ratings$pred, test_set$rating)
```

```{r table_3, echo = FALSE, warning=FALSE}
result_table <- rbind(result_table,
                data_frame(Method = "Movie + User Effect", RMSE = RMSE_user))
result_table %>% knitr::kable(caption = "RMSEs of models")
```

Adding the user effect to the model gives a further significant improvement compared to the previous model.

Our analysis showed, that the release year also could have some impact for the calculation. It's implemeted as follows:
```{r user_release_year_model, warning=FALSE}
# Release year effect without regularization
b_y <- train_set %>%
  left_join(b_i, by = "movieId") %>%
  left_join(b_u, by = "userId") %>%
  group_by(year) %>%
  summarize(b_y = mean(rating - mu - b_i - b_u))

# Predict ratings
predicted_ratings <- test_set %>%
  left_join(b_i, by = "movieId") %>%
  left_join(b_u, by = "userId") %>%
  left_join(b_y, by = "year") %>%
  mutate(pred = mu + b_i + b_u + b_y)

# Calculate the RMSE
RMSE_release_year <- RMSE(predicted_ratings$pred, test_set$rating)
```

```{r table_4, echo = FALSE, warning=FALSE}
result_table <- rbind(result_table,
                data_frame(Method = "Movie + User + Release Year Effect", RMSE = RMSE_release_year))
result_table %>% knitr::kable(caption = "RMSEs of models")
```

In this case the improvement is much smaller, but considering the fact, that the overall rating for RMSE in this project is very narrowly splitted it's seems worth the implementation.

Taking the genre into consideration is a bit more difficult. Herefore we need the splitted models which we created earlier. These models contain more than one row per rating if more than one genre was listed in the original dataset. With help of the splitted datasets we then calculate the genre effect and predict the ratings. Now in some cases we have more than one prediction for a certain ratingId. To make our final prediction we therefore have to take the average predicted rating per ratingId and make it our final prediction.
```{r user_genre_model, warning=FALSE}
# Genre effect without regularization
b_g <- train_set_split %>%
  left_join(b_i, by = "movieId") %>%
  left_join(b_u, by = "userId") %>%
  left_join(b_y, by = "year") %>%
  group_by(genres) %>%
  summarize(b_g = mean(rating - mu - b_i - b_u - b_y))

# Predict ratings  
predicted_ratings <- test_set_split %>%
  left_join(b_i, by = "movieId") %>%
  left_join(b_u, by = "userId") %>%
  left_join(b_y, by = "year") %>%
  left_join(b_g, by = "genres") %>%
  mutate(pred = mu + b_i + b_u + b_y + b_g)

# Combine predictions by taking the mean of the predicted rating per ratingId  
predicted_ratings <- predicted_ratings %>% group_by(ratingId) %>%
  summarize(pred = mean(pred))

# Calculate the RMSE
RMSE_genre <- RMSE(predicted_ratings$pred, test_set$rating)
```

```{r table_5, echo = FALSE, warning=FALSE}
result_table <- rbind(result_table,
                data_frame(Method = "Movie + User + Release Year + Genre Effect", RMSE = RMSE_genre))
result_table %>% knitr::kable(caption = "RMSEs of models")
```

As you can see we again improve our model, although the effort for this improvement seems very high. Nevertheless we will keep these four variables and see what improvement we can get through regularization.

### 3.3 Models with regularization
In this section we will follow the same process as in the previous chapter but regualarize the data. We define the vector **lambdas** which contains the test values for the penalization term of the regularization:

```{r lambdas, warning=FALSE}
# Vector for the tuning parameters
lambdas <- seq(0, 10, 0.25)
```

Then we will use the sapply function to optimize the result by minimizing the RMSEs. 
Movie model:
```{r movie_effect_model_reg, warning=FALSE}
# Compute the RMSE for all elements in our lambdas vector
rmses <- sapply(lambdas, function(l){
  
  # Mean value of the rating for the train_set
  mu <- mean(train_set$rating)
  
  # Movie effect including regularization 
  b_i <- train_set %>%
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  
  # Predict the rating for the test set
  predicted_ratings <- test_set %>%
    left_join(b_i, by = "movieId") %>%
    mutate(pred = mu + b_i) %>%
    pull(pred)
  
  # Return the RMSE between prediction and real rating
  return(RMSE(predicted_ratings, test_set$rating))
})

# Storing the lowest RMSE and the associated lambda
RMSE_movie_reg <- min(rmses)
lambda_movie <- lambdas[which.min(rmses)]
```
```{r table_6, echo = FALSE, warning=FALSE}
result_table <- rbind(result_table,
                data_frame(Method = "Movie Effect Reg.", RMSE = RMSE_movie_reg))
```

Adding the user effect:
```{r user_effect_model_reg, warning=FALSE}
# Compute the RMSE for all elements in our lambdas vector
rmses <- sapply(lambdas, function(l){
  
  # Mean value of the rating for the train_set
  mu <- mean(train_set$rating)
  
  # Movie effect including regularization 
  b_i <- train_set %>%
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  
  # User effect including regularization 
  b_u <- train_set %>%
    left_join(b_i, by = "movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - mu - b_i)/(n()+l))
  
  # Predict the rating for the test set
  predicted_ratings <- test_set %>%
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = mu + b_i + b_u) %>%
    pull(pred)
  
  # Return the RMSE between prediction and real rating
  return(RMSE(predicted_ratings, test_set$rating))
})

# Storing the lowest RMSE and the associated lambda
RMSE_user_reg <- min(rmses)
lambda_user <- lambdas[which.min(rmses)]
```
```{r table_7, echo = FALSE, warning=FALSE}
result_table <- rbind(result_table,
                data_frame(Method = "Movie + User Effect Reg.", RMSE = RMSE_user_reg))
```

The release year effect:
```{r release_year_effect_model_reg, warning=FALSE}
# Compute the RMSE for all elements in our lambdas vector
rmses <- sapply(lambdas, function(l){
  
  # Mean value of the rating for the train_set
  mu <- mean(train_set$rating)
  
  # Movie effect including regularization 
  b_i <- train_set %>%
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  
  # User effect including regularization 
  b_u <- train_set %>%
    left_join(b_i, by = "movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - mu - b_i)/(n()+l))
  
  # Release year effect including regularization
  b_y <- train_set %>%
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    group_by(year) %>%
    summarize(b_y = sum(rating - mu - b_i - b_u)/(n()+l))
  
  # Predict the rating for the test set
  predicted_ratings <- test_set %>%
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    left_join(b_y, by = "year") %>%
    mutate(pred = mu + b_i + b_u + b_y) %>%
    pull(pred)
  
  # Return the RMSE between prediction and real rating
  return(RMSE(predicted_ratings, test_set$rating))
})

# Storing the lowest RMSE and the associated lambda
RMSE_release_year_reg <- min(rmses)
lambda_release_year <- lambdas[which.min(rmses)]
```
```{r table_8, echo = FALSE, warning=FALSE}
result_table <- rbind(result_table,
                data_frame(Method = "Movie + User + Release Year Effect Reg.", RMSE = RMSE_release_year_reg))
```

And finally the genre, which is as mentioned in the previous section calculated based on the split data:
```{r genre_effect_model_reg, warning=FALSE}
# Compute the RMSE for all elements in our lambdas vector
rmses <- sapply(lambdas, function(l){
  
  # Mean value of the rating for the train_set
  mu <- mean(train_set$rating)
  
  # Movie effect including regularization 
  b_i <- train_set %>%
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  
  # User effect including regularization 
  b_u <- train_set %>%
    left_join(b_i, by = "movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - mu - b_i)/(n()+l))
  
  # Release year effect including regularization
  b_y <- train_set %>%
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    group_by(year) %>%
    summarize(b_y = sum(rating - mu - b_i - b_u)/(n()+l))
  
  # Genre effect based on the split data including regularization
  b_g <- train_set_split %>%
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    left_join(b_y, by = "year") %>%
    group_by(genres) %>%
    summarize(b_g = sum(rating - mu - b_i - b_u - b_y)/(n()+l))
  
  # Predict the rating for the splitted test set
  predicted_ratings <- test_set_split %>%
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    left_join(b_y, by = "year") %>%
    left_join(b_g, by = "genres") %>%
    mutate(pred = mu + b_i + b_u + b_y + b_g)
  
  # Taking the mean per ratingId
  predicted_ratings <- predicted_ratings %>% group_by(ratingId) %>%
    summarize(pred = mean(pred), rating = mean(rating))

  # Return the RMSE between prediction and real rating
  return(RMSE(predicted_ratings$pred, predicted_ratings$rating))
})

# Storing the lowest RMSE and the associated lambda
RMSE_genre_reg <- min(rmses)
lambda_genre <- lambdas[which.min(rmses)]
```

We can take a look at the optimization parameter for the full model by plotting lambda against RMSE:
```{r plot_lambda}
qplot(lambdas, rmses,
      main = "Effect of regularization on RMSE",
      xlab = "Lambda", ylab ="RMSE")
```  


```{r table_9, echo = FALSE, warning=FALSE}
result_table <- rbind(result_table,
                data_frame(Method = "Movie + User + Release Year + Genre Effect Reg.", RMSE = RMSE_genre_reg))
result_table %>% knitr::kable(caption = "RMSEs of models")
```

The table above summarizes the results of the different models. As we can see, the best model is the one taking all four variables into consideration and regularizing all the terms. This is also the model we will now use for the validation dataset.

## 4. Results
For validating the model we will calculate the factors on the whole edx dataset using the optimized lambda from the full dataset above. The program looks exactly the same as above, excluding the optimization loop of course.
```{r validation, warning=FALSE}
### Using the whole edx dataset for the building of the new model 
### which we will then test on the validation set
# Mean value of the rating for the whole edx set
lambda <- lambda_genre
mu <- mean(edx$rating)

# Movie effect including regularization 
b_i <- edx %>%
  group_by(movieId) %>%
  summarize(b_i = sum(rating - mu)/(n()+lambda))

# User effect including regularization 
b_u <- edx %>%
  left_join(b_i, by = "movieId") %>%
  group_by(userId) %>%
  summarize(b_u = sum(rating - mu - b_i)/(n()+lambda))

# Release year effect including regularization 
b_y <- edx %>%
  left_join(b_i, by = "movieId") %>%
  left_join(b_u, by = "userId") %>%
  group_by(year) %>%
  summarize(b_y = sum(rating - mu - b_i - b_u)/(n()+lambda))

# Genre effect based on the split data including regularization
b_g <- edx_split %>%
  left_join(b_i, by = "movieId") %>%
  left_join(b_u, by = "userId") %>%
  left_join(b_y, by = "year") %>%
  group_by(genres) %>%
  summarize(b_g = sum(rating - mu - b_i - b_u - b_y)/(n()+lambda))

# Predict the rating for the validation set
predicted_ratings <- validation
# Seperate by genre
predicted_ratings <- predicted_ratings %>% separate_rows(genres, sep = "\\|")

# Prediction for the split data
predicted_ratings <- predicted_ratings %>%
  left_join(b_i, by = "movieId") %>%
  left_join(b_u, by = "userId") %>%
  left_join(b_y, by = "year") %>%
  left_join(b_g, by = "genres") %>%
  mutate(pred = mu + b_i + b_u + b_y +b_g)

# Summarizing the data to get one prediction per ratingId
predicted_ratings <- predicted_ratings %>% group_by(ratingId) %>%
  summarize(pred = mean(pred))

# Calculation the RMSE on the validation set
RMSE_validation <- RMSE(predicted_ratings$pred, validation$rating)
```

```{r table_10, echo = FALSE, warning=FALSE}
result_table <- rbind(result_table,
                data_frame(Method = "Validation - full model", RMSE = RMSE_validation))
result_table %>% knitr::kable(caption = "RMSEs of models")
```

As we can see, the model achieves our set target to get an RMSE < 0.8649, by using the optimized $\lambda$ value of `r lambda_genre`. Using the full edx dataset for a final calculation of the parameters for the optimized approach leads to an even better result on the validation set than on the test done before.

The model building process in the section before shows a very consistent improvement by adding in more variables for the model bulding. The most significant improvements were made using **movieId** and **userId** and the model accuracy was raised even more useing **release year** and **genre**. The improvement achieved with these variables was much smaller.

Futher improvements were made by implementing regularization to the model and optimizing the penelization term. The direction of the results were consistent with the ones optained without this term. The RMSEs while using the regularization were better/lower than without.

Testing the optimized model on the validation set (the model was redefined before using the whole edx dataset with the optimized lambda for the best model) brought the targeted result.

## 5. Conclusion, limitations and comments
The model build during this project was complex in the end. Most of the complexity was added by the need to split up the genre column due to multiple assignments. Allocating every movie to just one genre would reduce the effort significantly.
We can also see, that a majority of the accuracy can be achieved by a very small effort, the unregularized model just using the movie and user effect achieved an RMSE of `r RMSE_user` compared to the final result of `r RMSE_validation`. The computational effort added for this minimal improvement was huge.

These methods are very basic in my point of view although they achieve an accuracy which is sufficient for maximum points on the grading scale. Trying more complex/ressource intensive (RAM) methods was unfortunately not possible. Setting up the **ratingMatrix** with the **recommenderlab package** (required 5.5 GB of RAM), but further computation was not possible due to hardware limitations. I wanted to try out matrix factorization (SVD / PCA) in order to identify patterns in movie or user groups and use them for further redefining the model. Unfortunately it was not possible.

First of all, I really liked the project and the challenges that came with it. Going step by step through some of the things learned during the pervious courses and applying them by trial and error was really fun. Maybe it would be better to just use the 1 Mio. dataset for this project and set the bar higher in terms of grading, making it possible and mandatory to implement more advanced methods. It's a real shame that I could dig in deeper when the best part was about to come due to the afore mentioned limitations.

Hope you enjoyed the project and this report as well and maybe got some new insights you can use in the future.